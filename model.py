# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KFRPUkozWV5CUT0jOVFgjJMw9q2acSm0

### Import Libraries
"""

import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Input, Add, UpSampling2D, Conv2D, Conv2DTranspose, BatchNormalization, Activation, Lambda, Flatten, GlobalAveragePooling2D, Dense, Concatenate
from keras.callbacks import TensorBoard

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

"""### Generator & Discriminator Architecture"""

def dense_block(x, growth_rate=32):
    inputs = x
    concat_feats = [x]

    for _ in range(3):
        out = Conv2D(growth_rate, 3, padding='same', activation='relu')(x)
        concat_feats.append(out)
        x = Concatenate()(concat_feats)

    # Final compress to match input channel count
    x = Conv2D(inputs.shape[-1], 3, padding='same')(x)
    return Add()([inputs, x])

def rrdb_block(x, scaling=0.2):
    input_x = x
    for _ in range(3):  # 3 dense blocks inside 1 RRDB
        x = dense_block(x)
    x = Lambda(lambda x: x * scaling)(x)  # scaled residual
    return Add()([input_x, x])

def generator_model():
    """Generator architecture with RRDB integration."""
    inputs = tf.keras.layers.Input(shape=(256, 256, 3))
    x = inputs

    # Initial Convolution
    x = layers.Conv2D(64, kernel_size=(7, 7), padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    # Downsampling
    for filters in [128, 256]:
        x = Conv2D(filters, kernel_size=(3, 3), strides=2, padding='same')(x)
        x = BatchNormalization()(x)
        x = Activation('relu')(x)

    # RRDB Blocks (replaces standard residual blocks)
    for _ in range(2):  # Use 2 RRDBs for balance
        x = rrdb_block(x)

    # Upsampling
    for filters in [128, 64]:
        x = layers.UpSampling2D(interpolation='nearest')(x)
        x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)
        x = BatchNormalization()(x)
        x = layers.ReLU()(x)

    # Final output
    x = Conv2D(3, kernel_size=(7, 7), padding='same')(x)
    x = Activation('tanh')(x)

    model = Model(inputs=inputs, outputs=x, name='Generator')
    return model

    # --- Discriminator Model ---
def discriminator_model():
    inputs = tf.keras.layers.Input(shape=(256, 256, 3))
    x = Conv2D(32, kernel_size=(4, 4), strides=2, padding='same')(inputs)
    x = layers.LeakyReLU(alpha=0.2)(x)

    for filters in [64, 128, 256, 512]:
        x = Conv2D(filters, kernel_size=(4, 4), strides=2, padding='same')(x)
        x = BatchNormalization()(x)
        x = layers.LeakyReLU(alpha=0.2)(x)

    # Progressively increases filters (64 -> 128 -> 256 -> 512)
    # 3 downsampling layers (PatchGAN architecture)

    # x = Flatten()(x)
    x = GlobalAveragePooling2D()(x)

    x = Dense(128, activation='tanh')(x)
    x = Dense(1, activation='sigmoid')(x)

    model = Model(inputs=inputs, outputs=x, name='Discriminator')
    return model

    # --- Full GAN Model (Generator + Discriminator) ---
def generator_containing_discriminator(generator, discriminator):
    inputs = tf.keras.layers.Input(shape=(256, 256, 3))
    generated_image = generator(inputs)
    outputs = discriminator(generated_image)
    model = Model(inputs=inputs, outputs=outputs)
    return model

generator = generator_model()
discriminator = discriminator_model()

# Print model summaries
generator.summary()
discriminator.summary()

"""#### END"""