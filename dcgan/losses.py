# -*- coding: utf-8 -*-
"""losses.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sppMR_GO-hF1Nyz3_12XpxCj3zbN8bgq
"""

# Loss Functions
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras import backend as K

vgg = VGG16(include_top=False, weights='imagenet', input_shape=(256, 256, 3))
loss_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)
loss_model.trainable = False

# Perceptual Loss (Using Pretrained VGG16)
def perceptual_loss(y_true, y_pred):
    # Normalize Inputs to Match VGG16 Preprocessing
    y_true_resized = tf.keras.applications.vgg16.preprocess_input((y_true + 1) * 127.5)
    y_pred_resized = tf.keras.applications.vgg16.preprocess_input((y_pred + 1) * 127.5)

    y_true_features = loss_model(y_true_resized)
    y_pred_features = loss_model(y_pred_resized)

    return K.mean(K.square(y_true_features - y_pred_features))

def generator_loss(fake_output, generated_images, sharp_images):
    wasserstein_loss = -tf.reduce_mean(fake_output)  # WGAN loss

    perceptual = perceptual_loss(tf.clip_by_value(sharp_images, -1.0, 1.0),
                                 tf.clip_by_value(generated_images, -1.0, 1.0)) # Perceptual loss

    l1_loss = tf.reduce_mean(tf.abs(sharp_images - generated_images))  # L1 Loss

    return wasserstein_loss + (1 * perceptual) + (5 * l1_loss)

# This method quantifies how well the generator was able to trick the discriminator.

def gradient_penalty(discriminator, sharp_images, generated_images):
    """
    Computes gradient penalty for WGAN-GP, ensuring numerical stability.
    """
    batch_size = tf.shape(sharp_images)[0]

    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)  # Alpha shape must match image shape

    # Correct interpolation (batch_size, 256, 256, 3)
    interpolated = (alpha * sharp_images) + ((1 - alpha) * generated_images)

    with tf.GradientTape() as tape:
        tape.watch(interpolated)
        prediction = discriminator(interpolated, training=True)  # Pass interpolated image, not outputs

    gradients = tape.gradient(prediction, [interpolated])[0]

    # Clip Gradients to Prevent Exploding Values
    gradients = tf.clip_by_value(gradients, -1, 1)  # Reduce from -1e6 to a reasonable range

    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))

    # Avoid Division by Zero (Add Small Epsilon)
    gradient_penalty = tf.reduce_mean((slopes - 1.0) ** 2) + 1e-8

    return gradient_penalty

# Discriminator Loss (WGAN loss)
def discriminator_loss(real_output, fake_output, sharp_images, generated_images):
    wasserstein_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)
    gp = gradient_penalty(discriminator, sharp_images, generated_images)
    return wasserstein_loss + 10 * gp  # Use lambda_gp=10.0

# This method quantifies how well the discriminator is able to distinguish real images from fakes.
# It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s.

"""#### END"""